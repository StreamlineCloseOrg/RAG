{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "import tiktoken\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.vectorstores import Pinecone as LangChainPinecone\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get env variables\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "# Construct the path to the .env file in the parent directory\n",
    "env_path = os.path.join(current_directory, '..', '.env')\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.abspath(os.getcwd())\n",
    "relative_path = \"../data/\" # if a data folder is in the parent directory\n",
    "filename = \"your_data_here.pdf\"\n",
    "file_path = os.path.join(current_dir, relative_path, filename)\n",
    "print(current_dir)\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_data(file_path):\n",
    "    # Initialize the PyMuPDFLoader\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    # Load the documents from the specified file_path\n",
    "    docs = loader.load()\n",
    "\n",
    "    data = []\n",
    "    for doc in docs:\n",
    "        tmp_dict = {}\n",
    "        tmp_dict['text'] = doc.page_content.replace('\\n', ' ')\n",
    "        tmp_dict['page'] = doc.metadata['page']\n",
    "        tmp_dict['title'] = doc.metadata['title']\n",
    "        data.append(tmp_dict)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_page_data(file_path)\n",
    "for entry in data:\n",
    "    print(f\"Page: {entry['page']}\")\n",
    "\n",
    "    print(f\"Title: {entry['title']}\")\n",
    "    # Split text by period or newline characters and print each sentence on a new line\n",
    "    lines = entry['text'].replace(' ● ', '\\n● ').split('. ')\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "  \n",
    "# tokenizer setup\n",
    "tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings setup using OpenAI\n",
    "model_name = 'text-embedding-ada-002'\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.exceptions import PineconeApiException \n",
    "\n",
    "# Create an instance of the Pinecone class\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Prepare the index name\n",
    "index_name = (\n",
    "    data[0]['title']\n",
    "    .lower()                              # Convert to lowercase\n",
    "    .replace(' ', '-')                    # Replace spaces with hyphens\n",
    ")\n",
    "\n",
    "# Remove all characters except lowercase letters, numbers, and hyphens\n",
    "index_name = re.sub(r'[^a-z0-9\\-]', '', index_name)[:45]\n",
    "\n",
    "# List current indexes to ensure the check is accurate\n",
    "current_indexes = pc.list_indexes()\n",
    "\n",
    "if index_name not in current_indexes:\n",
    "    try:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536, # 1536 dim of text-embedding-ada-002\n",
    "            metric=\"cosine\", # Replace with your model metric\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            ) \n",
    "        )\n",
    "    except PineconeApiException as e:\n",
    "        if \"ALREADY_EXISTS\" in str(e):\n",
    "            print(f\"Index '{index_name}' already exists.\")\n",
    "        else:\n",
    "            raise\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Access the index using the Index class\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_in_batches(data, text_splitter, embed, index, batch_limit=100):\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for i, record in enumerate(tqdm(data)):\n",
    "        # first get metadata fields for this record\n",
    "        metadata = {\n",
    "            'title': record['title'],\n",
    "            'page': record['page'],\n",
    "        }\n",
    "        # now we create chunks from the record text\n",
    "        record_texts = text_splitter.split_text(record['text'])\n",
    "        # create individual metadata dicts for each chunk\n",
    "        record_metadatas = [{\n",
    "            \"chunk\": j, \"text\": text, **metadata\n",
    "        } for j, text in enumerate(record_texts)]\n",
    "        # append these to current batches\n",
    "        texts.extend(record_texts)\n",
    "        metadatas.extend(record_metadatas)\n",
    "        # if we have reached the batch_limit we can add texts\n",
    "        if len(texts) >= batch_limit:\n",
    "            ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "            embeds = embed.embed_documents(texts)\n",
    "            index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "            texts = []\n",
    "            metadatas = []\n",
    "\n",
    "    if len(texts) > 0:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "        \n",
    "process_data_in_batches(data, text_splitter, embed, index, batch_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving vectorstore from pinecone\n",
    "text_field = \"text\"\n",
    "# switch back to normal index for langchain\n",
    "index = pc.Index(index_name)\n",
    "vectorstore = LangChainPinecone(\n",
    "    index=index,\n",
    "    embedding=embed.embed_query,  # The function or object to generate embeddings\n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query retrieval\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "question = \"Write your question here\"\n",
    "response = qa(question)\n",
    "answer=response['result']\n",
    "pprint.pprint(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-testing-P8xkvbMm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
